{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sakaya/ml-env/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/Users/sakaya/ml-env/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import custom_layers\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter setup\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "num_samples = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST data (5000 train, 5000 val and 10000 test) with 50% noise in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (5000, 784)\n",
      "5000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# # Load and preprocess MNIST data\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# x_train = x_train.reshape([-1, 28*28])\n",
    "# x_test = x_test.reshape([-1, 28*28])\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "\n",
    "# y_train = y_train.astype('int32')\n",
    "# y_test = y_test.astype('int32')\n",
    "\n",
    "# train_samples = x_train.shape[0]\n",
    "# print(x_train.shape[0], 'train samples')\n",
    "# print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# x_val, y_val = x_test, y_test\n",
    "n_samples = 5000\n",
    "I = np.eye(num_classes)\n",
    "x_train_S, y_train_S, x_test, y_test = utilities.load_mnist(n_samples, square=False, conv=False)\n",
    "y_train_S = y_train_S.astype('int32')\n",
    "y_test = y_test.astype('int32')\n",
    "\n",
    "N = int(n_samples/2)\n",
    "\n",
    "x_val = x_train_S[N:]\n",
    "y_val = y_train_S[N:]\n",
    "\n",
    "x_train = x_train_S[:N]\n",
    "y_train = y_train_S[:N]\n",
    "\n",
    "# Add output noise to important class\n",
    "R = int(.5*N)\n",
    "r_ind = random.sample(range(0,N),R)\n",
    "r_class = np.random.randint(0,num_classes,R)\n",
    "y_train[r_ind] = r_class\n",
    "\n",
    "train_samples = N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         TRUTH\n",
      "\n",
      "P    0 :  [1.e+00 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "R    1 :  [1.e-04 1.e+00 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "E    2 :  [1.e-04 1.e-04 1.e+00 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "D    3 :  [0.3 0.3 0.3 1.  0.3 0.3 0.3 0.3 0.3 0.3]\n",
      "     4 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e+00 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "     5 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e+00 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "     6 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e+00 1.e-04 1.e-04 1.e-04]\n",
      "     7 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e+00 1.e-04 1.e-04]\n",
      "     8 :  [0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 1.  0.3]\n",
      "     9 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e+00]\n",
      "[[0.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  0.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  0.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.7 0.7 0.7 0.  0.7 0.7 0.7 0.7 0.7 0.7]\n",
      " [1.  1.  1.  1.  0.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  0.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  0.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  0.  1.  1. ]\n",
      " [0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.  0.7]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "digits = [3,8]\n",
    "np.set_printoptions(precision=3)\n",
    "L = np.copy(I)\n",
    "L[np.where(I==0)] = 1\n",
    "L[[3,8]] = 0.7 # Select more important rows with lower loss in prediction\n",
    "L[np.where(I==1)] = 0\n",
    "\n",
    "loss_mat = L\n",
    "M = 1.0001\n",
    "\n",
    "print(\"         TRUTH\\n\" )\n",
    "string = 'PRED'\n",
    "for i in range(num_classes):\n",
    "    if i < 4:\n",
    "        print(string[i],'  ',str(i),': ',M - loss_mat[i])\n",
    "    else:\n",
    "        print('    ',str(i),': ',M - loss_mat[i])\n",
    "        \n",
    "# Set up weight for weighted cross entropy\n",
    "\n",
    "class_weight = np.ones((num_classes))\n",
    "class_weight[digits] = 1.4\n",
    "\n",
    "print(loss_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "\n",
    "logits = custom_layers.Softmax( \n",
    "    10, \n",
    "    kernel_divergence_fn=scaled_kl)([x_input, y_input])\n",
    "\n",
    "model_sm = tf.keras.Model([x_input, y_input], logits)\n",
    "model_sm.compile(loss=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_model_sm = []\n",
    "for epoch in range(epochs):\n",
    "    history_epoch = model_sm.fit([x_train, y_train],\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=1,\n",
    "                 verbose=1,\n",
    "                 validation_data=([x_val, y_val], None))\n",
    "#                  callbacks=[model_checkpoint])\n",
    "#     T = 10\n",
    "#     yt_hat = np.array([lc_model.predict([x_train,H_x]) for _ in range(T)])\n",
    "#     H_x = lcbnn.optimal_h(yt_hat,loss_mat)\n",
    "#     h_train = np.argmax(model.predict([x_train, y_train, h_train]), -1)\n",
    "    history_model_sm.append(history_epoch)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model_sm.predict([x_test,y_test]), 1)\n",
    "print(\"Test accuracy: \", len(np.where(y_pred - y_test == 0)[0])/len(y_test))\n",
    "\n",
    "y_pred = np.argmax(model_sm.predict([x_train, y_train]), 1)\n",
    "print(\"Train accuracy: \", len(np.where(y_pred - y_train == 0)[0])/len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss calibrated softmax loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproduceable results\n",
    "importlib.reload(custom_layers)\n",
    "importlib.reload(utilities)\n",
    "\n",
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "h_input =  tf.keras.Input((1,), name = 'h_input')\n",
    "\n",
    "logits = custom_layers.LCSoftmax( \n",
    "    10, \n",
    "    M,\n",
    "    loss_mat,\n",
    "    kernel_divergence_fn=scaled_kl)([x_input, y_input, h_input])\n",
    "\n",
    "model_sm_lc = tf.keras.Model([x_input, y_input, h_input], logits)\n",
    "model_sm_lc.compile(loss=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_train = y_train\n",
    "h_test = y_test\n",
    "h_val = y_val\n",
    "\n",
    "history_model_sm_lc = []\n",
    "for epoch in range(epochs):\n",
    "    history_epoch = model_sm_lc.fit([x_train, y_train, h_train],\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=1,\n",
    "                 verbose=1,\n",
    "                 validation_data=([x_val, y_val, h_val], None))\n",
    "#                  callbacks=[model_checkpoint])\n",
    "    T = 10\n",
    "    yt_hat = np.array([model_sm_lc.predict([x_train, y_train, h_train]) for _ in range(T)])\n",
    "    h_train = utilities.optimal_h(yt_hat, loss_mat)\n",
    "    history_model_sm_lc.append(history_epoch)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-vs-each loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproduceable results\n",
    "importlib.reload(custom_layers)\n",
    "importlib.reload(utilities)\n",
    "# np.random.seed(1337)\n",
    "\n",
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "\n",
    "logits = custom_layers.OVELayer(\n",
    "    5,\n",
    "    10, \n",
    "    kernel_divergence_fn=scaled_kl)([x_input, y_input])\n",
    "\n",
    "model_ove = tf.keras.Model([x_input, y_input], logits)\n",
    "model_ove.compile(loss=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_model_ove = []\n",
    "for epoch in range(epochs):\n",
    "    history_epoch = model_ove.fit([x_train, y_train],\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=1,\n",
    "                 verbose=1,\n",
    "                 validation_data=([x_val, y_val], None))\n",
    "#                  callbacks=[model_checkpoint])\n",
    "\n",
    "    history_model_ove.append(history_epoch)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproduceable results\n",
    "importlib.reload(custom_layers)\n",
    "importlib.reload(utilities)\n",
    "# np.random.seed(1337)\n",
    "\n",
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "\n",
    "logits = custom_layers.NCE(\n",
    "    5,\n",
    "    10, \n",
    "    kernel_divergence_fn=scaled_kl)([x_input, y_input])\n",
    "\n",
    "model_nce = tf.keras.Model([x_input, y_input], logits)\n",
    "model_nce.compile(loss=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_model_nce = []\n",
    "for epoch in range(200):\n",
    "    history_epoch = model_nce.fit([x_train, y_train],\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=1,\n",
    "                 verbose=1,\n",
    "                 validation_data=([x_val, y_val], None))\n",
    "#                  callbacks=[model_checkpoint])\n",
    "\n",
    "    history_model_nce.append(history_epoch)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Softmax model\n",
    "\n",
    "T = 10\n",
    "yt_hat_std = np.array([model_sm.predict([x_test, y_test]) for _ in range(T)])\n",
    "H_x_test_std = utilities.optimal_h(yt_hat_std,loss_mat) \n",
    "acc_std = accuracy_score(y_test, H_x_test_std)\n",
    "loss_std = np.mean(loss_mat[y_test, H_x_test_std])\n",
    "print('Standard:\\n')\n",
    "print('Accuracy on optimal decision: ', acc_std)\n",
    "print('Expected loss: ', loss_std)\n",
    "\n",
    "\n",
    "# #  Softmax with loss calibration\n",
    "\n",
    "# T = 10\n",
    "# yt_hat_sm_lc = np.array([model_sm_lc.predict([x_test, y_test, h_test]) for _ in range(T)])\n",
    "# H_x_test_sm_lc = utilities.optimal_h(yt_hat_sm_lc,loss_mat) \n",
    "# acc_sm_lc = accuracy_score(y_test, H_x_test_sm_lc)\n",
    "# loss_sm_lc = np.mean(loss_mat[y_test, H_x_test_sm_lc])\n",
    "# print('\\nLoss calibrated softmax:\\n')\n",
    "# print('Accuracy on optimal decision: ', acc_sm_lc)\n",
    "# print('Expected loss: ', loss_sm_lc)\n",
    "\n",
    "\n",
    "#Â OVE model\n",
    "\n",
    "T = 10 \n",
    "yt_hat_ove = np.array([model_ove.predict([x_test, y_test]) for _ in range(T)])\n",
    "H_x_test_ove = utilities.optimal_h(yt_hat_ove,loss_mat) \n",
    "acc_ove = accuracy_score(y_test, H_x_test_ove)\n",
    "loss_ove = np.mean(loss_mat[y_test, H_x_test_ove])\n",
    "print('\\nOVE:\\n')\n",
    "print('Accuracy on optimal decision: ', acc_ove)\n",
    "print('Expected loss: ', loss_ove)\n",
    "\n",
    "\n",
    "\n",
    "# NCE model\n",
    "\n",
    "T = 10 \n",
    "yt_hat_nce = np.array([model_nce.predict([x_test, y_test]) for _ in range(T)])\n",
    "H_x_test_nce = utilities.optimal_h(yt_hat_nce,loss_mat) \n",
    "acc_nce = accuracy_score(y_test, H_x_test_nce)\n",
    "loss_nce = np.mean(loss_mat[y_test, H_x_test_nce])\n",
    "print('\\nNCE:\\n')\n",
    "print('Accuracy on optimal decision: ', acc_nce)\n",
    "print('Expected loss: ', loss_nce)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10, 10) # Width and height\n",
    "\n",
    "classes = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "\n",
    "cnf_matrix_std = confusion_matrix(y_test.astype('float'),np.round(H_x_test_std))\n",
    "np.set_printoptions(precision=2)\n",
    "cnf_matrix_sm_lc = confusion_matrix(y_test.astype('float'),np.round(H_x_test_sm_lc))\n",
    "np.set_printoptions(precision=2)\n",
    "cnf_matrix_ove = confusion_matrix(y_test.astype('float'),np.round(H_x_test_ove))\n",
    "np.set_printoptions(precision=2)\n",
    "cnf_matrix_nce = confusion_matrix(y_test.astype('float'),np.round(H_x_test_nce))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.subplot(221)\n",
    "utilities.plot_confusion_matrix(cnf_matrix_std, classes=classes, normalize=True,\n",
    "                      title='Softmax')\n",
    "plt.subplot(222)\n",
    "utilities.plot_confusion_matrix(cnf_matrix_sm_lc, classes=classes, normalize=True,\n",
    "                      title='LC Softmax')\n",
    "plt.subplot(223)\n",
    "utilities.plot_confusion_matrix(cnf_matrix_ove, classes=classes, normalize=True,\n",
    "                      title='OVE')\n",
    "plt.subplot(224)\n",
    "utilities.plot_confusion_matrix(cnf_matrix_nce, classes=classes, normalize=True,\n",
    "                      title='NCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         TRUTH\n",
      "\n",
      "P    0 :  [1.e+00 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "R    1 :  [1.e-04 1.e+00 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "E    2 :  [1.e-04 1.e-04 1.e+00 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "D    3 :  [1.e-04 1.e-04 1.e-04 1.e+00 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "     4 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e+00 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "     5 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e+00 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "     6 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e+00 1.e-04 1.e-04 1.e-04]\n",
      "     7 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e+00 1.e-04 1.e-04]\n",
      "     8 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e+00 1.e-04]\n",
      "     9 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e+00]\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "digits = [3,8]\n",
    "np.set_printoptions(precision=3)\n",
    "L = np.copy(I)\n",
    "L[np.where(I==0)] = 1\n",
    "# L[[3,8]] = 0.7 # Select more important rows with lower loss in prediction\n",
    "L[np.where(I==1)] = 0\n",
    "\n",
    "loss_mat = L\n",
    "M = 1.0001\n",
    "\n",
    "print(\"         TRUTH\\n\" )\n",
    "string = 'PRED'\n",
    "for i in range(num_classes):\n",
    "    if i < 4:\n",
    "        print(string[i],'  ',str(i),': ',M - loss_mat[i])\n",
    "    else:\n",
    "        print('    ',str(i),': ',M - loss_mat[i])\n",
    "        \n",
    "# Set up weight for weighted cross entropy\n",
    "\n",
    "class_weight = np.ones((num_classes))\n",
    "class_weight[digits] = 1.4\n",
    "\n",
    "print(loss_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic loss calibration for softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output \"slc_softmax_61\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"slc_softmax_61\".\n"
     ]
    }
   ],
   "source": [
    "# For reproduceable results\n",
    "importlib.reload(custom_layers)\n",
    "importlib.reload(utilities)\n",
    "\n",
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "h_input =  tf.keras.Input((1,), name = 'h_input')\n",
    "\n",
    "outputs = x_input\n",
    "# outputs = tfp.layers.DenseReparameterization(50,\n",
    "#             activation = tf.nn.relu,\n",
    "#             kernel_divergence_fn=scaled_kl)(x_input)\n",
    "logits = custom_layers.SLCSoftmax( \n",
    "    10, \n",
    "    M,\n",
    "    loss_mat,\n",
    "    kernel_divergence_fn=scaled_kl)([outputs, y_input, h_input])\n",
    "\n",
    "model_sm_slc = tf.keras.Model([x_input, y_input, h_input], logits)\n",
    "model_sm_slc.compile(loss=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 21.4099\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 32us/step - loss: 21.5469\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 21.2130\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 21.1271\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 21.0693\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 30us/step - loss: 20.9794\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 21.0267\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 20.7030\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 20.9027\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 20.9068\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 33us/step - loss: 20.7087\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 31us/step - loss: 20.4830\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 28us/step - loss: 20.6238\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 20.5249\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 28us/step - loss: 20.5704\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 30us/step - loss: 20.6699\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 20.2992\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 28us/step - loss: 20.5198\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 32us/step - loss: 20.7087\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 20.1457\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 20.5925\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 32us/step - loss: 20.0691\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 20.4933\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 31us/step - loss: 19.7632\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 30us/step - loss: 19.8927\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 30us/step - loss: 19.7610\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 31us/step - loss: 20.0078\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 20.2533\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 28us/step - loss: 20.0256\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 31us/step - loss: 19.9713\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 28us/step - loss: 19.8194\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 27us/step - loss: 19.8330\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 20.0095\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 19.8165\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 32us/step - loss: 19.8065\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 28us/step - loss: 20.1381\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 30us/step - loss: 19.7185\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 32us/step - loss: 19.8287\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 30us/step - loss: 19.2751\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 30us/step - loss: 19.6558\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 32us/step - loss: 19.5277\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 31us/step - loss: 19.5519\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 19.7064\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 32us/step - loss: 19.3770\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 19.5413\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 31us/step - loss: 19.1057\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 31us/step - loss: 19.5686\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 28us/step - loss: 19.2607\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 33us/step - loss: 18.9943\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 19.1875\n"
     ]
    }
   ],
   "source": [
    "yt_hat = np.expand_dims(I[y_train],0)\n",
    "h_train = utilities.optimal_h(yt_hat, loss_mat)\n",
    "h_test = y_test\n",
    "h_val = y_val\n",
    "\n",
    "history_model_sm_slc = []\n",
    "for epoch in range(50):\n",
    "    history_epoch = model_sm_slc.fit([x_train, y_train, h_train],\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=1,\n",
    "                 verbose=1)\n",
    "#                  validation_data=([x_val, y_val, h_val], None))\n",
    "#                  callbacks=[model_checkpoint])\n",
    "    T = 1\n",
    "    yt_hat = np.array([model_sm_slc.predict([x_train, y_train, h_train]) for _ in range(T)])\n",
    "    h_train = utilities.optimal_h(yt_hat, loss_mat)\n",
    "    history_model_sm_slc.append(history_epoch)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss calibrated softmax:\n",
      "\n",
      "Accuracy on optimal decision:  0.6211\n",
      "Expected loss:  0.3789\n"
     ]
    }
   ],
   "source": [
    "#  Softmax with stochastic loss calibration\n",
    "T = 10\n",
    "yt_hat_sm_slc = np.array([model_sm_slc.predict([x_test, y_test, h_test]) for _ in range(T)])\n",
    "H_x_test_sm_slc = utilities.optimal_h(yt_hat_sm_slc,loss_mat) \n",
    "acc_sm_slc = accuracy_score(y_test, H_x_test_sm_slc)\n",
    "loss_sm_slc = np.mean(loss_mat[y_test, H_x_test_sm_slc])\n",
    "print('\\nLoss calibrated softmax:\\n')\n",
    "print('Accuracy on optimal decision: ', acc_sm_slc)\n",
    "print('Expected loss: ', loss_sm_slc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output \"softmax_7\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"softmax_7\".\n"
     ]
    }
   ],
   "source": [
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "\n",
    "outputs = x_input\n",
    "# output = tfp.layers.DenseReparameterization(50,\n",
    "#             activation = tf.nn.relu,\n",
    "#             kernel_divergence_fn=scaled_kl)(x_input)\n",
    "logits = custom_layers.Softmax( \n",
    "    10, \n",
    "    kernel_divergence_fn=scaled_kl)([outputs, y_input])\n",
    "\n",
    "model_sm = tf.keras.Model([x_input, y_input], logits)\n",
    "model_sm.compile(loss=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 10.5328 - val_loss: 10.1912\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 68us/step - loss: 10.2078 - val_loss: 9.7954\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 52us/step - loss: 10.0203 - val_loss: 9.4954\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 50us/step - loss: 9.9080 - val_loss: 9.3234\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 9.7962 - val_loss: 9.1808\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 9.7117 - val_loss: 9.0730\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 77us/step - loss: 9.6426 - val_loss: 8.9620\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 109us/step - loss: 9.5629 - val_loss: 8.8895\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 50us/step - loss: 9.4937 - val_loss: 8.8131\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 74us/step - loss: 9.4501 - val_loss: 8.7452\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 9.3898 - val_loss: 8.7118\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 50us/step - loss: 9.3136 - val_loss: 8.6523\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 59us/step - loss: 9.2873 - val_loss: 8.5959\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 50us/step - loss: 9.2071 - val_loss: 8.5614\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 9.1688 - val_loss: 8.4774\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 9.1067 - val_loss: 8.4151\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 9.0596 - val_loss: 8.3742\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 8.9977 - val_loss: 8.3448\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 8.9412 - val_loss: 8.2751\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 108us/step - loss: 8.9105 - val_loss: 8.2570\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 87us/step - loss: 8.8506 - val_loss: 8.1830\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 89us/step - loss: 8.8266 - val_loss: 8.1541\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 63us/step - loss: 8.7523 - val_loss: 8.0891\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 191us/step - loss: 8.7145 - val_loss: 8.0487\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 8.6717 - val_loss: 8.0197\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 74us/step - loss: 8.6208 - val_loss: 7.9799\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 89us/step - loss: 8.5659 - val_loss: 7.9208\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 81us/step - loss: 8.5526 - val_loss: 7.8747\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 66us/step - loss: 8.4696 - val_loss: 7.8578\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 68us/step - loss: 8.4122 - val_loss: 7.7984\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 55us/step - loss: 8.3937 - val_loss: 7.7478\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 55us/step - loss: 8.3354 - val_loss: 7.7139\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 69us/step - loss: 8.3015 - val_loss: 7.6562\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 134us/step - loss: 8.2605 - val_loss: 7.6288\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 100us/step - loss: 8.1896 - val_loss: 7.5802\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 173us/step - loss: 8.1605 - val_loss: 7.5376\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 56us/step - loss: 8.1087 - val_loss: 7.5319\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 86us/step - loss: 8.0548 - val_loss: 7.4566\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 105us/step - loss: 8.0254 - val_loss: 7.4135\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 7.9767 - val_loss: 7.3853\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 1s 228us/step - loss: 7.9463 - val_loss: 7.3268\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 146us/step - loss: 7.8896 - val_loss: 7.3202\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 139us/step - loss: 7.8215 - val_loss: 7.2865\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 106us/step - loss: 7.8364 - val_loss: 7.2222\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 1s 220us/step - loss: 7.7910 - val_loss: 7.1899\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 109us/step - loss: 7.7069 - val_loss: 7.1555\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 54us/step - loss: 7.6936 - val_loss: 7.1184\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 67us/step - loss: 7.6377 - val_loss: 7.0637\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 61us/step - loss: 7.5922 - val_loss: 7.0010\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 55us/step - loss: 7.5372 - val_loss: 6.9975\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history_model_sm = []\n",
    "for epoch in range(50):\n",
    "    history_epoch = model_sm.fit([x_train, y_train],\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=1,\n",
    "                 verbose=1,\n",
    "                 validation_data=([x_val, y_val], None))\n",
    "#                  callbacks=[model_checkpoint])\n",
    "#     T = 10\n",
    "#     yt_hat = np.array([lc_model.predict([x_train,H_x]) for _ in range(T)])\n",
    "#     H_x = lcbnn.optimal_h(yt_hat,loss_mat)\n",
    "#     h_train = np.argmax(model.predict([x_train, y_train, h_train]), -1)\n",
    "    history_model_sm.append(history_epoch)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard:\n",
      "\n",
      "Accuracy on optimal decision:  0.7605\n",
      "Expected loss:  0.2395\n"
     ]
    }
   ],
   "source": [
    "T = 10\n",
    "yt_hat_std = np.array([model_sm.predict([x_test, y_test]) for _ in range(T)])\n",
    "H_x_test_std = utilities.optimal_h(yt_hat_std,loss_mat) \n",
    "acc_std = accuracy_score(y_test, H_x_test_std)\n",
    "loss_std = np.mean(loss_mat[y_test, H_x_test_std])\n",
    "print('Standard:\\n')\n",
    "print('Accuracy on optimal decision: ', acc_std)\n",
    "print('Expected loss: ', loss_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output \"ar_bound_5\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"ar_bound_5\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output \"ar_bound_5\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"ar_bound_5\".\n"
     ]
    }
   ],
   "source": [
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "importlib.reload(custom_layers)\n",
    "importlib.reload(utilities)\n",
    "\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "\n",
    "outputs = x_input\n",
    "# output = tfp.layers.DenseReparameterization(50,\n",
    "#             activation = tf.nn.relu,\n",
    "#             kernel_divergence_fn=scaled_kl)(x_input)\n",
    "logits = custom_layers.ARBound( \n",
    "    5,\n",
    "    10, \n",
    "    kernel_divergence_fn=scaled_kl)([outputs, y_input])\n",
    "\n",
    "model_ar = tf.keras.Model([x_input, y_input], logits)\n",
    "model_ar.compile(loss=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 4s 1ms/step - loss: 10.4042\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 49us/step - loss: 10.2416\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 10.0009\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 9.8806\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 9.8123\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 9.6970\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 9.6234\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 9.5694\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 9.4733\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 52us/step - loss: 9.4282\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 73us/step - loss: 9.3501\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 74us/step - loss: 9.2971\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 118us/step - loss: 9.2707\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 9.1679\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 61us/step - loss: 9.1652\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 9.0402\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 71us/step - loss: 9.0181\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 52us/step - loss: 9.0194\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 50us/step - loss: 8.9686\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 8.8569\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 54us/step - loss: 8.7765\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 8.7922\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 67us/step - loss: 8.7489\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 68us/step - loss: 8.6902\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 88us/step - loss: 8.6186\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 85us/step - loss: 8.5712\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 83us/step - loss: 8.6270\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 107us/step - loss: 8.5189\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 63us/step - loss: 8.4324\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 80us/step - loss: 8.4395\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 54us/step - loss: 8.3654\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 56us/step - loss: 8.3450\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 57us/step - loss: 8.3455\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 62us/step - loss: 8.2299\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 67us/step - loss: 8.1640\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 56us/step - loss: 8.2415\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 64us/step - loss: 8.1632\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 62us/step - loss: 8.0745\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 108us/step - loss: 8.0934\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 78us/step - loss: 7.9860\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 89us/step - loss: 7.9344\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 72us/step - loss: 7.9006\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 71us/step - loss: 7.8302\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 67us/step - loss: 7.8987\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 58us/step - loss: 7.7923\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 57us/step - loss: 7.7756\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 61us/step - loss: 7.7523\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 52us/step - loss: 7.6961\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 52us/step - loss: 7.6577\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 55us/step - loss: 7.5889\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history_model_ar = []\n",
    "for epoch in range(50):\n",
    "    history_epoch = model_ar.fit([x_train, y_train],\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=1,\n",
    "                 verbose=1)\n",
    "#                  validation_data=([x_val, y_val], None))\n",
    "#                  callbacks=[model_checkpoint])\n",
    "#     T = 10\n",
    "#     yt_hat = np.array([lc_model.predict([x_train,H_x]) for _ in range(T)])\n",
    "#     H_x = lcbnn.optimal_h(yt_hat,loss_mat)\n",
    "#     h_train = np.argmax(model.predict([x_train, y_train, h_train]), -1)\n",
    "    history_model_ar.append(history_epoch)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard:\n",
      "\n",
      "Accuracy on optimal decision:  0.7605\n",
      "Expected loss:  0.2342\n"
     ]
    }
   ],
   "source": [
    "T = 10\n",
    "yt_hat_ar = np.array([model_ar.predict([x_test, y_test]) for _ in range(T)])\n",
    "H_x_test_ar = utilities.optimal_h(yt_hat_ar,loss_mat) \n",
    "acc_ar = accuracy_score(y_test, H_x_test_ar)\n",
    "loss_ar = np.mean(loss_mat[y_test, H_x_test_ar])\n",
    "print('Standard:\\n')\n",
    "print('Accuracy on optimal decision: ', acc_std)\n",
    "print('Expected loss: ', loss_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
