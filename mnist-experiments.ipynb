{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import new_custom_layers\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter setup\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "num_samples = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST data (5000 train, 5000 val and 10000 test) with 50% noise in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (5000, 784)\n",
      "5000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# # Load and preprocess MNIST data\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# x_train = x_train.reshape([-1, 28*28])\n",
    "# x_test = x_test.reshape([-1, 28*28])\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "\n",
    "# y_train = y_train.astype('int32')\n",
    "# y_test = y_test.astype('int32')\n",
    "\n",
    "# train_samples = x_train.shape[0]\n",
    "# print(x_train.shape[0], 'train samples')\n",
    "# print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# x_val, y_val = x_test, y_test\n",
    "n_samples = 5000\n",
    "I = np.eye(num_classes)\n",
    "x_train_S, y_train_S, x_test, y_test = utilities.load_mnist(n_samples, square=False, conv=False)\n",
    "y_train_S = y_train_S.astype('int32')\n",
    "y_test = y_test.astype('int32')\n",
    "\n",
    "N = int(n_samples/2)\n",
    "\n",
    "x_val = x_train_S[N:]\n",
    "y_val = y_train_S[N:]\n",
    "\n",
    "x_train = x_train_S[:N]\n",
    "y_train = y_train_S[:N]\n",
    "\n",
    "# Add output noise to important class\n",
    "R = int(.5*N)\n",
    "r_ind = random.sample(range(0,N),R)\n",
    "r_class = np.random.randint(0,num_classes,R)\n",
    "y_train[r_ind] = r_class\n",
    "\n",
    "train_samples = N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         TRUTH\n",
      "\n",
      "P    0 :  [1.e+00 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "R    1 :  [1.e-04 1.e+00 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "E    2 :  [1.e-04 1.e-04 1.e+00 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "D    3 :  [1.e-04 1.e-04 1.e-04 1.e+00 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "     4 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e+00 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "     5 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e+00 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "     6 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e+00 1.e-04 1.e-04 1.e-04]\n",
      "     7 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e+00 1.e-04 1.e-04]\n",
      "     8 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e+00 1.e-04]\n",
      "     9 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e+00]\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "digits = [3,8]\n",
    "np.set_printoptions(precision=3)\n",
    "L = np.copy(I)\n",
    "L[np.where(I==0)] = 1\n",
    "# L[[3,8]] = 0.7 # Select more important rows with lower loss in prediction\n",
    "L[np.where(I==1)] = 0\n",
    "\n",
    "loss_mat = L\n",
    "M = 1.0001\n",
    "\n",
    "print(\"         TRUTH\\n\" )\n",
    "string = 'PRED'\n",
    "for i in range(num_classes):\n",
    "    if i < 4:\n",
    "        print(string[i],'  ',str(i),': ',M - loss_mat[i])\n",
    "    else:\n",
    "        print('    ',str(i),': ',M - loss_mat[i])\n",
    "        \n",
    "# Set up weight for weighted cross entropy\n",
    "\n",
    "class_weight = np.ones((num_classes))\n",
    "class_weight[digits] = 1.4\n",
    "\n",
    "print(loss_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output \"softmax_1\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"softmax_1\".\n"
     ]
    }
   ],
   "source": [
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "\n",
    "logits = custom_layers.Softmax( \n",
    "    10, \n",
    "    kernel_divergence_fn=scaled_kl)([x_input, y_input])\n",
    "\n",
    "model_sm = tf.keras.Model([x_input, y_input], logits)\n",
    "model_sm.compile(loss=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 1s 276us/sample - loss: 10.5079 - val_loss: 10.2027\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 32us/sample - loss: 10.1920 - val_loss: 9.8040\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 36us/sample - loss: 10.0016 - val_loss: 9.5019\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 48us/sample - loss: 9.8761 - val_loss: 9.3110\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 31us/sample - loss: 9.7879 - val_loss: 9.1557\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 29us/sample - loss: 9.7019 - val_loss: 9.0630\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 47us/sample - loss: 9.6267 - val_loss: 8.9494\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 30us/sample - loss: 9.5718 - val_loss: 8.8586\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 32us/sample - loss: 9.4758 - val_loss: 8.8153\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 34us/sample - loss: 9.4211 - val_loss: 8.7499\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 34us/sample - loss: 9.3953 - val_loss: 8.6939\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 31us/sample - loss: 9.3251 - val_loss: 8.6204\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 32us/sample - loss: 9.2795 - val_loss: 8.5760\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 30us/sample - loss: 9.2019 - val_loss: 8.5003\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 30us/sample - loss: 9.1714 - val_loss: 8.4919\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 44us/sample - loss: 9.1004 - val_loss: 8.4520\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 54us/sample - loss: 9.0724 - val_loss: 8.3743\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 55us/sample - loss: 9.0229 - val_loss: 8.3183\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 52us/sample - loss: 8.9683 - val_loss: 8.2868\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 32us/sample - loss: 8.9125 - val_loss: 8.2548\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 51us/sample - loss: 8.8741 - val_loss: 8.2065\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 85us/sample - loss: 8.8159 - val_loss: 8.1547\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 50us/sample - loss: 8.7855 - val_loss: 8.0898\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 45us/sample - loss: 8.7296 - val_loss: 8.0740\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 107us/sample - loss: 8.6699 - val_loss: 8.0409\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 72us/sample - loss: 8.6209 - val_loss: 7.9665\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 41us/sample - loss: 8.5912 - val_loss: 7.9351\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 55us/sample - loss: 8.5445 - val_loss: 7.8981\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 61us/sample - loss: 8.4896 - val_loss: 7.8535\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 53us/sample - loss: 8.4587 - val_loss: 7.8213\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 62us/sample - loss: 8.3840 - val_loss: 7.7590\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 62us/sample - loss: 8.3331 - val_loss: 7.7417\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 191us/sample - loss: 8.3088 - val_loss: 7.6895\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 98us/sample - loss: 8.2753 - val_loss: 7.6520\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 64us/sample - loss: 8.2042 - val_loss: 7.6097\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 53us/sample - loss: 8.1838 - val_loss: 7.5520\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 56us/sample - loss: 8.1256 - val_loss: 7.5284\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 41us/sample - loss: 8.0923 - val_loss: 7.4572\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 59us/sample - loss: 8.0192 - val_loss: 7.4279\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 50us/sample - loss: 7.9843 - val_loss: 7.4131\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 120us/sample - loss: 7.9376 - val_loss: 7.3568\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 66us/sample - loss: 7.9130 - val_loss: 7.3275\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 102us/sample - loss: 7.8794 - val_loss: 7.2736\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 71us/sample - loss: 7.8246 - val_loss: 7.2365\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 114us/sample - loss: 7.7720 - val_loss: 7.2132\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 69us/sample - loss: 7.7341 - val_loss: 7.1554\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 67us/sample - loss: 7.6998 - val_loss: 7.1142\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 84us/sample - loss: 7.6788 - val_loss: 7.0496\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 139us/sample - loss: 7.6157 - val_loss: 7.0401\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 76us/sample - loss: 7.5925 - val_loss: 6.9867\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 48us/sample - loss: 7.5353 - val_loss: 6.9740\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 60us/sample - loss: 7.4861 - val_loss: 6.9605\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 48us/sample - loss: 7.4465 - val_loss: 6.8877\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 61us/sample - loss: 7.3914 - val_loss: 6.8510\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 30us/sample - loss: 7.3784 - val_loss: 6.8127\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 29us/sample - loss: 7.3379 - val_loss: 6.7582\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 42us/sample - loss: 7.3083 - val_loss: 6.7524\n",
      "Train on 2500 samples, validate on 2500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 0s 51us/sample - loss: 7.2715 - val_loss: 6.7213\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 61us/sample - loss: 7.2220 - val_loss: 6.6944\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 53us/sample - loss: 7.1878 - val_loss: 6.6457\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 143us/sample - loss: 7.1463 - val_loss: 6.5962\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 84us/sample - loss: 7.1007 - val_loss: 6.5741\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 39us/sample - loss: 7.0877 - val_loss: 6.5408\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 29us/sample - loss: 7.0350 - val_loss: 6.4941\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 40us/sample - loss: 7.0094 - val_loss: 6.4502\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 34us/sample - loss: 6.9488 - val_loss: 6.4198\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 38us/sample - loss: 6.9381 - val_loss: 6.3906\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 51us/sample - loss: 6.8853 - val_loss: 6.3578\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 58us/sample - loss: 6.8528 - val_loss: 6.3407\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 30us/sample - loss: 6.8028 - val_loss: 6.2847\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 27us/sample - loss: 6.7940 - val_loss: 6.2794\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 27us/sample - loss: 6.7562 - val_loss: 6.2094\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 19us/sample - loss: 6.6964 - val_loss: 6.2056\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 26us/sample - loss: 6.6524 - val_loss: 6.1047\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 42us/sample - loss: 6.6034 - val_loss: 6.1344\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 29us/sample - loss: 6.6131 - val_loss: 6.0562\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 30us/sample - loss: 6.5988 - val_loss: 6.0966\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 26us/sample - loss: 6.5055 - val_loss: 6.0170\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 27us/sample - loss: 6.5002 - val_loss: 5.9623\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 35us/sample - loss: 6.4433 - val_loss: 5.9683\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 95us/sample - loss: 6.4409 - val_loss: 5.9340\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 74us/sample - loss: 6.4087 - val_loss: 5.9102\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 27us/sample - loss: 6.3524 - val_loss: 5.8649\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 25us/sample - loss: 6.3306 - val_loss: 5.8428\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 31us/sample - loss: 6.3006 - val_loss: 5.7805\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 25us/sample - loss: 6.2661 - val_loss: 5.7726\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 25us/sample - loss: 6.2591 - val_loss: 5.7591\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 34us/sample - loss: 6.2194 - val_loss: 5.7201\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 61us/sample - loss: 6.1897 - val_loss: 5.7190\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 33us/sample - loss: 6.1672 - val_loss: 5.6393\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 61us/sample - loss: 6.1076 - val_loss: 5.6248\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 70us/sample - loss: 6.0655 - val_loss: 5.6122\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 46us/sample - loss: 6.0734 - val_loss: 5.5821\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 26us/sample - loss: 6.0471 - val_loss: 5.5546\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 29us/sample - loss: 6.0066 - val_loss: 5.5218\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 26us/sample - loss: 5.9917 - val_loss: 5.4841\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 62us/sample - loss: 5.9361 - val_loss: 5.4580\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 79us/sample - loss: 5.9207 - val_loss: 5.4277\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 50us/sample - loss: 5.9200 - val_loss: 5.4025\n",
      "Train on 2500 samples, validate on 2500 samples\n",
      "2500/2500 [==============================] - 0s 33us/sample - loss: 5.8782 - val_loss: 5.3968\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history_model_sm = []\n",
    "for epoch in range(epochs):\n",
    "    history_epoch = model_sm.fit([x_train, y_train],\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=1,\n",
    "                 verbose=1,\n",
    "                 validation_data=([x_val, y_val], None))\n",
    "#                  callbacks=[model_checkpoint])\n",
    "#     T = 10\n",
    "#     yt_hat = np.array([lc_model.predict([x_train,H_x]) for _ in range(T)])\n",
    "#     H_x = lcbnn.optimal_h(yt_hat,loss_mat)\n",
    "#     h_train = np.argmax(model.predict([x_train, y_train, h_train]), -1)\n",
    "    history_model_sm.append(history_epoch)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model_sm.predict([x_test,y_test]), 1)\n",
    "print(\"Test accuracy: \", len(np.where(y_pred - y_test == 0)[0])/len(y_test))\n",
    "\n",
    "y_pred = np.argmax(model_sm.predict([x_train, y_train]), 1)\n",
    "print(\"Train accuracy: \", len(np.where(y_pred - y_train == 0)[0])/len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss calibrated softmax loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproduceable results\n",
    "importlib.reload(custom_layers)\n",
    "importlib.reload(utilities)\n",
    "\n",
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "h_input =  tf.keras.Input((1,), name = 'h_input')\n",
    "\n",
    "logits = custom_layers.LCSoftmax( \n",
    "    10, \n",
    "    M,\n",
    "    loss_mat,\n",
    "    kernel_divergence_fn=scaled_kl)([x_input, y_input, h_input])\n",
    "\n",
    "model_sm_lc = tf.keras.Model([x_input, y_input, h_input], logits)\n",
    "model_sm_lc.compile(loss=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_train = y_train\n",
    "h_test = y_test\n",
    "h_val = y_val\n",
    "\n",
    "history_model_sm_lc = []\n",
    "for epoch in range(epochs):\n",
    "    history_epoch = model_sm_lc.fit([x_train, y_train, h_train],\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=1,\n",
    "                 verbose=1,\n",
    "                 validation_data=([x_val, y_val, h_val], None))\n",
    "#                  callbacks=[model_checkpoint])\n",
    "    T = 10\n",
    "    yt_hat = np.array([model_sm_lc.predict([x_train, y_train, h_train]) for _ in range(T)])\n",
    "    h_train = utilities.optimal_h(yt_hat, loss_mat)\n",
    "    history_model_sm_lc.append(history_epoch)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-vs-each loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproduceable results\n",
    "importlib.reload(custom_layers)\n",
    "importlib.reload(utilities)\n",
    "# np.random.seed(1337)\n",
    "\n",
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "\n",
    "logits = custom_layers.OVELayer(\n",
    "    5,\n",
    "    10, \n",
    "    kernel_divergence_fn=scaled_kl)([x_input, y_input])\n",
    "\n",
    "model_ove = tf.keras.Model([x_input, y_input], logits)\n",
    "model_ove.compile(loss=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_model_ove = []\n",
    "for epoch in range(epochs):\n",
    "    history_epoch = model_ove.fit([x_train, y_train],\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=1,\n",
    "                 verbose=1,\n",
    "                 validation_data=([x_val, y_val], None))\n",
    "#                  callbacks=[model_checkpoint])\n",
    "\n",
    "    history_model_ove.append(history_epoch)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproduceable results\n",
    "importlib.reload(custom_layers)\n",
    "importlib.reload(utilities)\n",
    "# np.random.seed(1337)\n",
    "\n",
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "\n",
    "logits = custom_layers.NCE(\n",
    "    5,\n",
    "    10, \n",
    "    kernel_divergence_fn=scaled_kl)([x_input, y_input])\n",
    "\n",
    "model_nce = tf.keras.Model([x_input, y_input], logits)\n",
    "model_nce.compile(loss=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_model_nce = []\n",
    "for epoch in range(200):\n",
    "    history_epoch = model_nce.fit([x_train, y_train],\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=1,\n",
    "                 verbose=1,\n",
    "                 validation_data=([x_val, y_val], None))\n",
    "#                  callbacks=[model_checkpoint])\n",
    "\n",
    "    history_model_nce.append(history_epoch)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Softmax model\n",
    "\n",
    "T = 10\n",
    "yt_hat_std = np.array([model_sm.predict([x_test, y_test]) for _ in range(T)])\n",
    "H_x_test_std = utilities.optimal_h(yt_hat_std,loss_mat) \n",
    "acc_std = accuracy_score(y_test, H_x_test_std)\n",
    "loss_std = np.mean(loss_mat[y_test, H_x_test_std])\n",
    "print('Standard:\\n')\n",
    "print('Accuracy on optimal decision: ', acc_std)\n",
    "print('Expected loss: ', loss_std)\n",
    "\n",
    "\n",
    "# #  Softmax with loss calibration\n",
    "\n",
    "# T = 10\n",
    "# yt_hat_sm_lc = np.array([model_sm_lc.predict([x_test, y_test, h_test]) for _ in range(T)])\n",
    "# H_x_test_sm_lc = utilities.optimal_h(yt_hat_sm_lc,loss_mat) \n",
    "# acc_sm_lc = accuracy_score(y_test, H_x_test_sm_lc)\n",
    "# loss_sm_lc = np.mean(loss_mat[y_test, H_x_test_sm_lc])\n",
    "# print('\\nLoss calibrated softmax:\\n')\n",
    "# print('Accuracy on optimal decision: ', acc_sm_lc)\n",
    "# print('Expected loss: ', loss_sm_lc)\n",
    "\n",
    "\n",
    "#Â OVE model\n",
    "\n",
    "T = 10 \n",
    "yt_hat_ove = np.array([model_ove.predict([x_test, y_test]) for _ in range(T)])\n",
    "H_x_test_ove = utilities.optimal_h(yt_hat_ove,loss_mat) \n",
    "acc_ove = accuracy_score(y_test, H_x_test_ove)\n",
    "loss_ove = np.mean(loss_mat[y_test, H_x_test_ove])\n",
    "print('\\nOVE:\\n')\n",
    "print('Accuracy on optimal decision: ', acc_ove)\n",
    "print('Expected loss: ', loss_ove)\n",
    "\n",
    "\n",
    "\n",
    "# NCE model\n",
    "\n",
    "T = 10 \n",
    "yt_hat_nce = np.array([model_nce.predict([x_test, y_test]) for _ in range(T)])\n",
    "H_x_test_nce = utilities.optimal_h(yt_hat_nce,loss_mat) \n",
    "acc_nce = accuracy_score(y_test, H_x_test_nce)\n",
    "loss_nce = np.mean(loss_mat[y_test, H_x_test_nce])\n",
    "print('\\nNCE:\\n')\n",
    "print('Accuracy on optimal decision: ', acc_nce)\n",
    "print('Expected loss: ', loss_nce)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10, 10) # Width and height\n",
    "\n",
    "classes = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "\n",
    "cnf_matrix_std = confusion_matrix(y_test.astype('float'),np.round(H_x_test_std))\n",
    "np.set_printoptions(precision=2)\n",
    "cnf_matrix_sm_lc = confusion_matrix(y_test.astype('float'),np.round(H_x_test_sm_lc))\n",
    "np.set_printoptions(precision=2)\n",
    "cnf_matrix_ove = confusion_matrix(y_test.astype('float'),np.round(H_x_test_ove))\n",
    "np.set_printoptions(precision=2)\n",
    "cnf_matrix_nce = confusion_matrix(y_test.astype('float'),np.round(H_x_test_nce))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.subplot(221)\n",
    "utilities.plot_confusion_matrix(cnf_matrix_std, classes=classes, normalize=True,\n",
    "                      title='Softmax')\n",
    "plt.subplot(222)\n",
    "utilities.plot_confusion_matrix(cnf_matrix_sm_lc, classes=classes, normalize=True,\n",
    "                      title='LC Softmax')\n",
    "plt.subplot(223)\n",
    "utilities.plot_confusion_matrix(cnf_matrix_ove, classes=classes, normalize=True,\n",
    "                      title='OVE')\n",
    "plt.subplot(224)\n",
    "utilities.plot_confusion_matrix(cnf_matrix_nce, classes=classes, normalize=True,\n",
    "                      title='NCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         TRUTH\n",
      "\n",
      "P    0 :  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "R    1 :  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "E    2 :  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "D    3 :  [0.3 0.3 0.3 1.  0.3 0.3 0.3 0.3 0.3 0.3]\n",
      "     4 :  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "     5 :  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "     6 :  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "     7 :  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "     8 :  [0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 1.  0.3]\n",
      "     9 :  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[[0.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  0.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  0.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.7 0.7 0.7 0.  0.7 0.7 0.7 0.7 0.7 0.7]\n",
      " [1.  1.  1.  1.  0.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  0.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  0.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  0.  1.  1. ]\n",
      " [0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.  0.7]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "digits = [3,8]\n",
    "np.set_printoptions(precision=3)\n",
    "L = np.copy(I)\n",
    "L[np.where(I==0)] = 1\n",
    "L[[3,8]] = 0.7 # Select more important rows with lower loss in prediction\n",
    "L[np.where(I==1)] = 0\n",
    "\n",
    "loss_mat = L\n",
    "M = 1.000\n",
    "\n",
    "print(\"         TRUTH\\n\" )\n",
    "string = 'PRED'\n",
    "util_mat = M - loss_mat\n",
    "for i in range(num_classes):\n",
    "    if i < 4:\n",
    "        print(string[i],'  ',str(i),': ',M - loss_mat[i])\n",
    "    else:\n",
    "        print('    ',str(i),': ',M - loss_mat[i])\n",
    "        \n",
    "# Set up weight for weighted cross entropy\n",
    "\n",
    "class_weight = np.ones((num_classes))\n",
    "class_weight[digits] = 1.4\n",
    "\n",
    "print(loss_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import new_custom_layers\n",
    "import custom_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic loss calibration for softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output \"ducar_layer_64\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"ducar_layer_64\".\n"
     ]
    }
   ],
   "source": [
    "# For reproduceable results\n",
    "importlib.reload(new_custom_layers)\n",
    "importlib.reload(utilities)\n",
    "\n",
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "\n",
    "outputs = x_input\n",
    "logits = new_custom_layers.DUCARLayer(\n",
    "    5,\n",
    "    num_classes, \n",
    "    util_mat,\n",
    "    kernel_divergence_fn=scaled_kl)([outputs, y_input])\n",
    "\n",
    "model_sm_slc = tf.keras.Model([x_input, y_input], logits)\n",
    "model_sm_slc.compile(loss=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 9.7865\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 0s 59us/step - loss: 9.8160\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 0s 52us/step - loss: 9.7189\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 0s 49us/step - loss: 9.7049\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 0s 53us/step - loss: 9.6569\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 9.5859\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 9.5106\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 0s 49us/step - loss: 9.4889\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 9.5324\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 0s 57us/step - loss: 9.4509\n"
     ]
    }
   ],
   "source": [
    "# history_model_sm_slc = []\n",
    "# for epoch in range(10):\n",
    "history_epoch = model_sm_slc.fit([x_train, y_train],\n",
    "             batch_size=batch_size,\n",
    "             epochs=10,\n",
    "             verbose=1)\n",
    "#                  validation_data=([x_val, y_val, h_val], None))\n",
    "#                  callbacks=[model_checkpoint])\n",
    "#     T = 1\n",
    "#     yt_hat = np.array([model_sm_slc.predict([x_train, y_train, h_train]) for _ in range(T)])\n",
    "#     h_train = utilities.optimal_h(yt_hat, loss_mat)\n",
    "#     history_model_sm_slc.append(history_epoch)\n",
    "       \n",
    "#     h_train = model_sm_slc.predict([x_train, y_train, h_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss calibrated softmax:\n",
      "\n",
      "Accuracy on optimal decision:  0.7352\n",
      "Expected loss:  0.24700999999999998\n"
     ]
    }
   ],
   "source": [
    "#  Softmax with stochastic loss calibration\n",
    "T = 10\n",
    "yt_hat_sm_slc = np.array([np.exp(model_sm_slc.predict([x_test, y_test])) for _ in range(T)])\n",
    "H_x_test_sm_slc = utilities.optimal_h(yt_hat_sm_slc,loss_mat) \n",
    "acc_sm_slc = accuracy_score(y_test, H_x_test_sm_slc)\n",
    "loss_sm_slc = np.mean(loss_mat[y_test, H_x_test_sm_slc])\n",
    "print('\\nLoss calibrated softmax:\\n')\n",
    "print('Accuracy on optimal decision: ', acc_sm_slc)\n",
    "print('Expected loss: ', loss_sm_slc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer ducar_layer_10 expects 3 inputs, but it received 2 input tensors. Inputs received: [<tf.Tensor 'x_input_52:0' shape=(?, 784) dtype=float32>, <tf.Tensor 'y_input_52:0' shape=(?, 1) dtype=float32>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-394-4b0bb410c29c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mutil_mat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     kernel_divergence_fn=scaled_kl)([outputs, y_input])\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel_sm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml-env/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml-env/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_assert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1453\u001b[0m                        \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' inputs, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m                        \u001b[0;34m'but it received '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1455\u001b[0;31m                        ' input tensors. Inputs received: ' + str(inputs))\n\u001b[0m\u001b[1;32m   1456\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minput_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer ducar_layer_10 expects 3 inputs, but it received 2 input tensors. Inputs received: [<tf.Tensor 'x_input_52:0' shape=(?, 784) dtype=float32>, <tf.Tensor 'y_input_52:0' shape=(?, 1) dtype=float32>]"
     ]
    }
   ],
   "source": [
    "importlib.reload(new_custom_layers)\n",
    "importlib.reload(utilities)\n",
    "\n",
    "\n",
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "\n",
    "outputs = x_input\n",
    "# output = tfp.layers.DenseReparameterization(50,\n",
    "#             activation = tf.nn.relu,\n",
    "#             kernel_divergence_fn=scaled_kl)(x_input)\n",
    "logits = custom_layers.DUCARLayer( \n",
    "    5,\n",
    "    num_classes, \n",
    "    M,\n",
    "    util_mat,\n",
    "    kernel_divergence_fn=scaled_kl)([outputs, y_input])\n",
    "\n",
    "model_sm = tf.keras.Model([x_input, y_input], logits)\n",
    "model_sm.compile(loss=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 7.2504\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 142us/step - loss: 7.2307\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 129us/step - loss: 7.1590\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 161us/step - loss: 7.1574\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 1s 206us/step - loss: 7.0899 0s - loss: 7.\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 1s 242us/step - loss: 7.0519\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 136us/step - loss: 7.0131\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 1s 222us/step - loss: 6.9944\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 1s 207us/step - loss: 6.9820\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 124us/step - loss: 6.9140\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history_model_sm = []\n",
    "for epoch in range(10):\n",
    "    history_epoch = model_sm.fit([x_train, y_train],\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=1,\n",
    "                 verbose=1)\n",
    "    history_model_sm.append(history_epoch)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard:\n",
      "\n",
      "Accuracy on optimal decision:  0.79\n",
      "Expected loss:  0.21\n"
     ]
    }
   ],
   "source": [
    "T = 10\n",
    "yt_hat_std = np.array([np.exp(model_sm.predict([x_test, y_test])) for _ in range(T)])\n",
    "H_x_test_std = utilities.optimal_h(yt_hat_std,loss_mat) \n",
    "acc_std = accuracy_score(y_test, H_x_test_std)\n",
    "loss_std = np.mean(loss_mat[y_test, H_x_test_std])\n",
    "print('Standard:\\n')\n",
    "print('Accuracy on optimal decision: ', acc_std)\n",
    "print('Expected loss: ', loss_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 0., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 0., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 0., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 0., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output \"ar_layer_5\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"ar_layer_5\".\n"
     ]
    }
   ],
   "source": [
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "importlib.reload(custom_layers)\n",
    "importlib.reload(utilities)\n",
    "\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "\n",
    "outputs = x_input\n",
    "# output = tfp.layers.DenseReparameterization(50,\n",
    "#             activation = tf.nn.relu,\n",
    "#             kernel_divergence_fn=scaled_kl)(x_input)\n",
    "logits = custom_layers.ARLayer(\n",
    "    5,\n",
    "    10,\n",
    "    kernel_divergence_fn=scaled_kl)([outputs, y_input])\n",
    "\n",
    "model_ar = tf.keras.Model([x_input, y_input], logits)\n",
    "model_ar.compile(loss=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 7.2256\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 122us/step - loss: 7.2233\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 94us/step - loss: 7.1472\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 111us/step - loss: 7.0855\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 193us/step - loss: 7.1392\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 120us/step - loss: 7.0206\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 117us/step - loss: 7.0012\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 98us/step - loss: 6.9478\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 113us/step - loss: 6.9387\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 101us/step - loss: 6.9261\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history_model_ar = []\n",
    "for epoch in range(10):\n",
    "    history_epoch = model_ar.fit([x_train, y_train],\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=1,\n",
    "                 verbose=1)\n",
    "#                  validation_data=([x_val, y_val], None))\n",
    "#                  callbacks=[model_checkpoint])\n",
    "#     T = 10\n",
    "#     yt_hat = np.array([lc_model.predict([x_train,H_x]) for _ in range(T)])\n",
    "#     H_x = lcbnn.optimal_h(yt_hat,loss_mat)\n",
    "#     h_train = np.argmax(model.predict([x_train, y_train, h_train]), -1)\n",
    "    history_model_ar.append(history_epoch)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard:\n",
      "\n",
      "Accuracy on optimal decision:  0.7929\n",
      "Expected loss:  0.2409\n"
     ]
    }
   ],
   "source": [
    "T = 10\n",
    "yt_hat_ar = np.array([(model_ar.predict([x_test, y_test])) for _ in range(T)])\n",
    "H_x_test_ar = utilities.optimal_h(yt_hat_ar,loss_mat) \n",
    "acc_ar = accuracy_score(y_test, H_x_test_ar)\n",
    "loss_ar = np.mean(loss_mat[y_test, H_x_test_ar])\n",
    "print('Standard:\\n')\n",
    "print('Accuracy on optimal decision: ', acc_std)\n",
    "print('Expected loss: ', loss_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.RaggedTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sakaya/loss-calibration/new_custom_layers.py:1672: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:Output \"ar_layer\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"ar_layer\".\n"
     ]
    }
   ],
   "source": [
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "\n",
    "logits = new_custom_layers.ARLayer( \n",
    "    5,\n",
    "    10, \n",
    "    kernel_divergence_fn=scaled_kl)([x_input, y_input])\n",
    "\n",
    "model_sm = tf.keras.Model([x_input, y_input], logits)\n",
    "model_sm.compile(loss=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 27us/step - loss: 7.9778\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 7.9859\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 7.8957\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 7.8621\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 7.8085\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 7.7841\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 7.7173\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 7.6742\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 25us/step - loss: 7.6717\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 7.6741\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history_model_sm = []\n",
    "for epoch in range(10):\n",
    "    history_epoch = model_sm.fit([x_train, y_train],\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=1,\n",
    "                 verbose=1)\n",
    "#                  validation_data=([x_val, y_val], None))\n",
    "#                  callbacks=[model_checkpoint])\n",
    "#     T = 10\n",
    "#     yt_hat = np.array([lc_model.predict([x_train,H_x]) for _ in range(T)])\n",
    "#     H_x = lcbnn.optimal_h(yt_hat,loss_mat)\n",
    "#     h_train = np.argmax(model.predict([x_train, y_train, h_train]), -1)\n",
    "    history_model_sm.append(history_epoch)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard:\n",
      "\n",
      "Accuracy on optimal decision:  0.6128\n",
      "Expected loss:  0.38081000000000004\n"
     ]
    }
   ],
   "source": [
    "T = 10\n",
    "yt_hat_std = np.array([np.exp(model_sm.predict([x_test, y_test])) for _ in range(T)])\n",
    "H_x_test_std = utilities.optimal_h(yt_hat_std,loss_mat) \n",
    "acc_std = accuracy_score(y_test, H_x_test_std)\n",
    "loss_std = np.mean(loss_mat[y_test, H_x_test_std])\n",
    "print('Standard:\\n')\n",
    "print('Accuracy on optimal decision: ', acc_std)\n",
    "print('Expected loss: ', loss_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output \"softmax_12\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"softmax_12\".\n"
     ]
    }
   ],
   "source": [
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "\n",
    "logits = new_custom_layers.Softmax( \n",
    "#     5,\n",
    "    10, \n",
    "    kernel_divergence_fn=scaled_kl)([x_input, y_input])\n",
    "\n",
    "model_sm = tf.keras.Model([x_input, y_input], logits)\n",
    "model_sm.compile(loss=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 88us/step - loss: 8.3663\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 77us/step - loss: 8.3486\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 8.2715\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 182us/step - loss: 8.2512\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 188us/step - loss: 8.2045\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 119us/step - loss: 8.1592\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 8.1182\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 158us/step - loss: 8.0538\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 179us/step - loss: 8.0079\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 92us/step - loss: 7.9730\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history_model_sm = []\n",
    "for epoch in range(10):\n",
    "    history_epoch = model_sm.fit([x_train, y_train],\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=1,\n",
    "                 verbose=1)\n",
    "#                  validation_data=([x_val, y_val], None))\n",
    "#                  callbacks=[model_checkpoint])\n",
    "#     T = 10\n",
    "#     yt_hat = np.array([lc_model.predict([x_train,H_x]) for _ in range(T)])\n",
    "#     H_x = lcbnn.optimal_h(yt_hat,loss_mat)\n",
    "#     h_train = np.argmax(model.predict([x_train, y_train, h_train]), -1)\n",
    "    history_model_sm.append(history_epoch)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard:\n",
      "\n",
      "Accuracy on optimal decision:  0.6255\n",
      "Expected loss:  0.36708999999999997\n"
     ]
    }
   ],
   "source": [
    "T = 10\n",
    "yt_hat_std = np.array([np.exp(model_sm.predict([x_test, y_test])) for _ in range(T)])\n",
    "H_x_test_std = utilities.optimal_h(yt_hat_std,loss_mat) \n",
    "acc_std = accuracy_score(y_test, H_x_test_std)\n",
    "loss_std = np.mean(loss_mat[y_test, H_x_test_std])\n",
    "print('Standard:\\n')\n",
    "print('Accuracy on optimal decision: ', acc_std)\n",
    "print('Expected loss: ', loss_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output \"ove_layer_10\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"ove_layer_10\".\n"
     ]
    }
   ],
   "source": [
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "importlib.reload(new_custom_layers)\n",
    "importlib.reload(utilities)\n",
    "\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "\n",
    "logits = new_custom_layers.OVELayer( \n",
    "    5,\n",
    "    10, \n",
    "#     util_mat,\n",
    "    kernel_divergence_fn=scaled_kl)([x_input, y_input])\n",
    "\n",
    "model_sm = tf.keras.Model([x_input, y_input], logits)\n",
    "model_sm.compile(loss=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 8.7206\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 0s 54us/step - loss: 8.6919\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 8.6384\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 0s 50us/step - loss: 8.6044\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 0s 57us/step - loss: 8.5519\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 0s 54us/step - loss: 8.4532\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 0s 50us/step - loss: 8.5448\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 8.5322\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 0s 38us/step - loss: 8.3514\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 8.3240\n"
     ]
    }
   ],
   "source": [
    "history_epoch = model_sm.fit([x_train, y_train],\n",
    "             batch_size=batch_size,\n",
    "             epochs=10,\n",
    "             verbose=1)\n",
    "#                  validation_data=([x_val, y_val], None))\n",
    "#                  callbacks=[model_checkpoint])\n",
    "#     T = 10\n",
    "#     yt_hat = np.array([lc_model.predict([x_train,H_x]) for _ in range(T)])\n",
    "#     H_x = lcbnn.optimal_h(yt_hat,loss_mat)\n",
    "#     h_train = np.argmax(model.predict([x_train, y_train, h_train]), -1)\n",
    "#     history_model_sm.append(history_epoch)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard:\n",
      "\n",
      "Accuracy on optimal decision:  0.549\n",
      "Expected loss:  0.44299000000000005\n"
     ]
    }
   ],
   "source": [
    "T = 10\n",
    "yt_hat_std = np.array([np.exp(model_sm.predict([x_test, y_test])) for _ in range(T)])\n",
    "H_x_test_std = utilities.optimal_h(yt_hat_std,loss_mat) \n",
    "acc_std = accuracy_score(y_test, H_x_test_std)\n",
    "loss_std = np.mean(loss_mat[y_test, H_x_test_std])\n",
    "print('Standard:\\n')\n",
    "print('Accuracy on optimal decision: ', acc_std)\n",
    "print('Expected loss: ', loss_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ],\n",
       "       [1. , 0. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ],\n",
       "       [1. , 1. , 0. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ],\n",
       "       [0.7, 0.7, 0.7, 0. , 0.7, 0.7, 0.7, 0.7, 0.7, 0.7],\n",
       "       [1. , 1. , 1. , 1. , 0. , 1. , 1. , 1. , 1. , 1. ],\n",
       "       [1. , 1. , 1. , 1. , 1. , 0. , 1. , 1. , 1. , 1. ],\n",
       "       [1. , 1. , 1. , 1. , 1. , 1. , 0. , 1. , 1. , 1. ],\n",
       "       [1. , 1. , 1. , 1. , 1. , 1. , 1. , 0. , 1. , 1. ],\n",
       "       [0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0. , 0.7],\n",
       "       [1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 0. ]])"
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ],\n",
       "       [1. , 0. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ],\n",
       "       [1. , 1. , 0. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ],\n",
       "       [0.7, 0.7, 0.7, 0. , 0.7, 0.7, 0.7, 0.7, 0.7, 0.7],\n",
       "       [1. , 1. , 1. , 1. , 0. , 1. , 1. , 1. , 1. , 1. ],\n",
       "       [0.7, 0.7, 0.7, 0.7, 0.7, 0. , 0.7, 0.7, 0.7, 0.7],\n",
       "       [1. , 1. , 1. , 1. , 1. , 1. , 0. , 1. , 1. , 1. ],\n",
       "       [1. , 1. , 1. , 1. , 1. , 1. , 1. , 0. , 1. , 1. ],\n",
       "       [0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0. , 0.7],\n",
       "       [1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 0. ]])"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'custom_layers' has no attribute 'SUCSoftmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-f15956d1a6fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mh_input\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'h_input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m logits = custom_layers.SUCSoftmax( \n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#     5,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'custom_layers' has no attribute 'SUCSoftmax'"
     ]
    }
   ],
   "source": [
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "importlib.reload(new_custom_layers)\n",
    "importlib.reload(utilities)\n",
    "\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "h_input =  tf.keras.Input((1,), name = 'h_input')\n",
    "\n",
    "logits = custom_layers.SUCSoftmax( \n",
    "#     5,\n",
    "    10, \n",
    "    util_mat,\n",
    "    kernel_divergence_fn=scaled_kl)([x_input, y_input, h_input])\n",
    "\n",
    "model_sm = tf.keras.Model([x_input, y_input, h_input], logits)\n",
    "model_sm.compile(loss=None, optimizer='adam')\n",
    "\n",
    "T = 1\n",
    "h_train = y_train[:]\n",
    "yt_hat = np.array([model_sm.predict([x_train, y_train, h_train]) for _ in range(T)])\n",
    "h_train = utilities.optimal_h(yt_hat, loss_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 7.6389\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 49us/step - loss: 7.6159\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 7.5512\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 56us/step - loss: 7.5797\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 38us/step - loss: 7.3148\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 51us/step - loss: 7.4312\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 7.2655\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 7.4425\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 50us/step - loss: 7.2806\n",
      "Epoch 1/1\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 7.2133\n"
     ]
    }
   ],
   "source": [
    "# history_model_sm_slc = []\n",
    "for epoch in range(10):\n",
    "    history_epoch = model_sm.fit([x_train, y_train, h_train],\n",
    "             batch_size=batch_size,\n",
    "             epochs=1,\n",
    "             verbose=1)\n",
    "    T = 1\n",
    "    yt_hat = np.array([model_sm.predict([x_train, y_train, h_train]) for _ in range(T)])\n",
    "    h_train = utilities.optimal_h(yt_hat, loss_mat)\n",
    "#     h_train = model_sm_slc.predict([x_train, y_train, h_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard:\n",
      "\n",
      "Accuracy on optimal decision:  0.3521\n",
      "Expected loss:  0.63515\n"
     ]
    }
   ],
   "source": [
    "T = 10\n",
    "yt_hat_std = np.array([np.exp(model_sm.predict([x_test, y_test])) for _ in range(T)])\n",
    "H_x_test_std = utilities.optimal_h(yt_hat_std,loss_mat) \n",
    "acc_std = accuracy_score(y_test, H_x_test_std)\n",
    "loss_std = np.mean(loss_mat[y_test, H_x_test_std])\n",
    "print('Standard:\\n')\n",
    "print('Accuracy on optimal decision: ', acc_std)\n",
    "print('Expected loss: ', loss_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [[i] for i in range(num_classes)]\n",
    "cols = [[i] for i in range(num_classes)]\n",
    "vals = [[1] for i in range(num_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_mat = 1 - np.eye(num_classes)\n",
    "loss_mat = np.eye(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 0., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 0., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 0., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 0., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output \"suc_softmax_28\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"suc_softmax_28\".\n"
     ]
    }
   ],
   "source": [
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "importlib.reload(new_custom_layers)\n",
    "importlib.reload(utilities)\n",
    "\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "\n",
    "logits = new_custom_layers.SUCSoftmax( \n",
    "#     5,\n",
    "    num_classes, \n",
    "    rows, cols, vals,\n",
    "#     util_mat,\n",
    "    kernel_divergence_fn=scaled_kl)([x_input, y_input])\n",
    "\n",
    "model_sm = tf.keras.Model([x_input, y_input], logits)\n",
    "model_sm.compile(loss=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 6s 3ms/sample - loss: 10.4040\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 0s 150us/sample - loss: 10.1202\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 0s 76us/sample - loss: 9.9511\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 0s 74us/sample - loss: 9.8696\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 0s 102us/sample - loss: 9.7705\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 0s 97us/sample - loss: 9.6960\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 0s 56us/sample - loss: 9.6136\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 0s 78us/sample - loss: 9.5542\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 0s 79us/sample - loss: 9.4983\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 0s 68us/sample - loss: 9.4234\n"
     ]
    }
   ],
   "source": [
    "history_epoch = model_sm.fit([x_train, y_train],\n",
    "         batch_size=batch_size,\n",
    "         epochs=10,\n",
    "         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard:\n",
      "\n",
      "Accuracy on optimal decision:  0.7476\n",
      "Expected loss:  0.2524\n"
     ]
    }
   ],
   "source": [
    "T = 10\n",
    "yt_hat_std = np.array([np.exp(model_sm.predict([x_test, y_test])) for _ in range(T)])\n",
    "H_x_test_std = utilities.optimal_h(yt_hat_std,loss_mat) \n",
    "acc_std = accuracy_score(y_test, H_x_test_std)\n",
    "loss_std = np.mean(loss_mat[y_test, H_x_test_std])\n",
    "print('Standard:\\n')\n",
    "print('Accuracy on optimal decision: ', acc_std)\n",
    "print('Expected loss: ', loss_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.887, -1.685, -0.058, ...,  2.011, -1.724,  0.278],\n",
       "       [-2.706, -3.272,  2.064, ..., -3.967,  0.85 , -2.42 ],\n",
       "       [-1.118,  1.594,  0.336, ..., -0.972, -0.248, -0.565],\n",
       "       ...,\n",
       "       [-0.244, -2.038, -1.755, ...,  0.011, -0.087,  0.439],\n",
       "       [-0.543, -1.571, -0.893, ...,  1.038,  0.584,  0.42 ],\n",
       "       [ 0.379, -1.515, -0.826, ...,  0.43 , -2.937, -1.965]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sm.predict([x_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [1], [1], [1], [1], [1], [1], [1], [1], [1]]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
