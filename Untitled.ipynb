{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sakaya/ml-env/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/Users/sakaya/ml-env/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utilities' from '/Users/sakaya/loss-calibration/utilities.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import custom_layers\n",
    "import utilities\n",
    "importlib.reload(utilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter setup\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "num_samples = 1\n",
    "\n",
    "n_samples = 5000 # number of MNIST samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (5000, 784)\n",
      "5000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "I = np.eye(num_classes)\n",
    "x_train_S, y_train_S, x_test, y_test = utilities.load_mnist(n_samples, square=False, conv=False)\n",
    "y_train_S = y_train_S.astype('int32')\n",
    "y_test = y_test.astype('int32')\n",
    "\n",
    "N = int(n_samples/2)\n",
    "\n",
    "x_val = x_train_S[N:]\n",
    "y_val = y_train_S[N:]\n",
    "\n",
    "x_train = x_train_S[:N]\n",
    "y_train = y_train_S[:N]\n",
    "\n",
    "# Add output noise to important class\n",
    "R = int(.5*N)\n",
    "r_ind = random.sample(range(0,N),R)\n",
    "r_class = np.random.randint(0,num_classes,R)\n",
    "y_train[r_ind] = r_class\n",
    "\n",
    "train_samples = N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         TRUTH\n",
      "\n",
      "P    0 :  [1.e+00 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "R    1 :  [1.e-04 1.e+00 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "E    2 :  [1.e-04 1.e-04 1.e+00 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "D    3 :  [0.3 0.3 0.3 1.  0.3 0.3 0.3 0.3 0.3 0.3]\n",
      "     4 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e+00 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "     5 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e+00 1.e-04 1.e-04 1.e-04 1.e-04]\n",
      "     6 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e+00 1.e-04 1.e-04 1.e-04]\n",
      "     7 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e+00 1.e-04 1.e-04]\n",
      "     8 :  [0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 1.  0.3]\n",
      "     9 :  [1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e-04 1.e+00]\n"
     ]
    }
   ],
   "source": [
    "digits = [3,8]\n",
    "np.set_printoptions(precision=3)\n",
    "L = np.copy(I)\n",
    "L[np.where(I==0)] = 1\n",
    "L[[3,8]] = 0.7 # Select more important rows with lower loss in prediction\n",
    "L[np.where(I==1)] = 0\n",
    "\n",
    "loss_mat = L\n",
    "M = 1.0001\n",
    "\n",
    "print(\"         TRUTH\\n\" )\n",
    "string = 'PRED'\n",
    "for i in range(num_classes):\n",
    "    if i < 4:\n",
    "        print(string[i],'  ',str(i),': ',M - loss_mat[i])\n",
    "    else:\n",
    "        print('    ',str(i),': ',M - loss_mat[i])\n",
    "        \n",
    "# Set up weight for weighted cross entropy\n",
    "\n",
    "class_weight = np.ones((num_classes))\n",
    "class_weight[digits] = 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproduceable results\n",
    "importlib.reload(custom_layers)\n",
    "importlib.reload(utilities)\n",
    "np.random.seed(1337)\n",
    "\n",
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "# Softmax layer\n",
    "def softmax_loss(labels, logits):\n",
    "    return tf.keras.backend.sparse_categorical_crossentropy(labels, logits, from_logits = True)\n",
    "  \n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "logits = tfp.layers.DenseReparameterization( \n",
    "    10,\n",
    "    kernel_divergence_fn=scaled_kl)(x_input)\n",
    "\n",
    "model = tf.keras.Model(x_input, logits)\n",
    "model.compile(loss=softmax_loss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 0s 194us/step - loss: 10.3894\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 0s 20us/step - loss: 10.1327\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 0s 31us/step - loss: 9.9653\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 0s 32us/step - loss: 9.8506\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 0s 33us/step - loss: 9.7375\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 0s 60us/step - loss: 9.6860\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 9.6053\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 0s 32us/step - loss: 9.5496\n",
      "Epoch 9/100\n",
      "2500/2500 [==============================] - 0s 33us/step - loss: 9.4817\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 0s 31us/step - loss: 9.4400\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 0s 32us/step - loss: 9.3679\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 0s 33us/step - loss: 9.3353\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 0s 31us/step - loss: 9.2358\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 0s 32us/step - loss: 9.2130\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 9.1455\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 0s 53us/step - loss: 9.1007\n",
      "Epoch 17/100\n",
      "2500/2500 [==============================] - 0s 52us/step - loss: 9.0617\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 8.9771\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 8.9337\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 8.9088\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 8.8676\n",
      "Epoch 22/100\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 8.7936\n",
      "Epoch 23/100\n",
      "2500/2500 [==============================] - 0s 30us/step - loss: 8.7598\n",
      "Epoch 24/100\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 8.7034\n",
      "Epoch 25/100\n",
      "2500/2500 [==============================] - 0s 32us/step - loss: 8.6467\n",
      "Epoch 26/100\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 8.5943\n",
      "Epoch 27/100\n",
      "2500/2500 [==============================] - 0s 72us/step - loss: 8.5619\n",
      "Epoch 28/100\n",
      "2500/2500 [==============================] - 0s 66us/step - loss: 8.5206\n",
      "Epoch 29/100\n",
      "2500/2500 [==============================] - 0s 72us/step - loss: 8.4894\n",
      "Epoch 30/100\n",
      "2500/2500 [==============================] - 0s 55us/step - loss: 8.4132\n",
      "Epoch 31/100\n",
      "2500/2500 [==============================] - 0s 58us/step - loss: 8.3815\n",
      "Epoch 32/100\n",
      "2500/2500 [==============================] - 0s 51us/step - loss: 8.3373\n",
      "Epoch 33/100\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 8.2825\n",
      "Epoch 34/100\n",
      "2500/2500 [==============================] - 0s 38us/step - loss: 8.2270\n",
      "Epoch 35/100\n",
      "2500/2500 [==============================] - 0s 38us/step - loss: 8.1913\n",
      "Epoch 36/100\n",
      "2500/2500 [==============================] - 0s 38us/step - loss: 8.1398\n",
      "Epoch 37/100\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 8.1076\n",
      "Epoch 38/100\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 8.0703\n",
      "Epoch 39/100\n",
      "2500/2500 [==============================] - 0s 32us/step - loss: 8.0215\n",
      "Epoch 40/100\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 7.9706\n",
      "Epoch 41/100\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 7.9277\n",
      "Epoch 42/100\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 7.8867\n",
      "Epoch 43/100\n",
      "2500/2500 [==============================] - 0s 38us/step - loss: 7.8500\n",
      "Epoch 44/100\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 7.8180\n",
      "Epoch 45/100\n",
      "2500/2500 [==============================] - 0s 79us/step - loss: 7.7548\n",
      "Epoch 46/100\n",
      "2500/2500 [==============================] - 0s 59us/step - loss: 7.7189\n",
      "Epoch 47/100\n",
      "2500/2500 [==============================] - 0s 32us/step - loss: 7.6652\n",
      "Epoch 48/100\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 7.6444\n",
      "Epoch 49/100\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 7.5899\n",
      "Epoch 50/100\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 7.5429\n",
      "Epoch 51/100\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 7.5248\n",
      "Epoch 52/100\n",
      "2500/2500 [==============================] - 0s 32us/step - loss: 7.4705\n",
      "Epoch 53/100\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 7.4378\n",
      "Epoch 54/100\n",
      "2500/2500 [==============================] - 0s 33us/step - loss: 7.4171\n",
      "Epoch 55/100\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 7.3432\n",
      "Epoch 56/100\n",
      "2500/2500 [==============================] - 0s 24us/step - loss: 7.3082\n",
      "Epoch 57/100\n",
      "2500/2500 [==============================] - 0s 19us/step - loss: 7.3060\n",
      "Epoch 58/100\n",
      "2500/2500 [==============================] - 0s 17us/step - loss: 7.2625\n",
      "Epoch 59/100\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 7.2241\n",
      "Epoch 60/100\n",
      "2500/2500 [==============================] - 0s 25us/step - loss: 7.1844\n",
      "Epoch 61/100\n",
      "2500/2500 [==============================] - 0s 24us/step - loss: 7.1246\n",
      "Epoch 62/100\n",
      "2500/2500 [==============================] - 0s 19us/step - loss: 7.0923\n",
      "Epoch 63/100\n",
      "2500/2500 [==============================] - 0s 18us/step - loss: 7.0649\n",
      "Epoch 64/100\n",
      "2500/2500 [==============================] - 0s 17us/step - loss: 7.0338\n",
      "Epoch 65/100\n",
      "2500/2500 [==============================] - 0s 25us/step - loss: 6.9960\n",
      "Epoch 66/100\n",
      "2500/2500 [==============================] - 0s 28us/step - loss: 6.9559\n",
      "Epoch 67/100\n",
      "2500/2500 [==============================] - 0s 60us/step - loss: 6.8848\n",
      "Epoch 68/100\n",
      "2500/2500 [==============================] - 0s 54us/step - loss: 6.8857\n",
      "Epoch 69/100\n",
      "2500/2500 [==============================] - 0s 49us/step - loss: 6.8584\n",
      "Epoch 70/100\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 6.7987\n",
      "Epoch 71/100\n",
      "2500/2500 [==============================] - 0s 33us/step - loss: 6.7766\n",
      "Epoch 72/100\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 6.7406\n",
      "Epoch 73/100\n",
      "2500/2500 [==============================] - 0s 18us/step - loss: 6.6861\n",
      "Epoch 74/100\n",
      "2500/2500 [==============================] - 0s 17us/step - loss: 6.6629\n",
      "Epoch 75/100\n",
      "2500/2500 [==============================] - 0s 19us/step - loss: 6.6004\n",
      "Epoch 76/100\n",
      "2500/2500 [==============================] - 0s 28us/step - loss: 6.5882\n",
      "Epoch 77/100\n",
      "2500/2500 [==============================] - 0s 25us/step - loss: 6.5592\n",
      "Epoch 78/100\n",
      "2500/2500 [==============================] - 0s 25us/step - loss: 6.5244\n",
      "Epoch 79/100\n",
      "2500/2500 [==============================] - 0s 23us/step - loss: 6.4753\n",
      "Epoch 80/100\n",
      "2500/2500 [==============================] - 0s 21us/step - loss: 6.4702\n",
      "Epoch 81/100\n",
      "2500/2500 [==============================] - 0s 17us/step - loss: 6.4324\n",
      "Epoch 82/100\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 6.3858\n",
      "Epoch 83/100\n",
      "2500/2500 [==============================] - 0s 24us/step - loss: 6.3344\n",
      "Epoch 84/100\n",
      "2500/2500 [==============================] - 0s 19us/step - loss: 6.3112\n",
      "Epoch 85/100\n",
      "2500/2500 [==============================] - 0s 17us/step - loss: 6.2868\n",
      "Epoch 86/100\n",
      "2500/2500 [==============================] - 0s 17us/step - loss: 6.2635\n",
      "Epoch 87/100\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 6.2068\n",
      "Epoch 88/100\n",
      "2500/2500 [==============================] - 0s 27us/step - loss: 6.2116\n",
      "Epoch 89/100\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 6.1789\n",
      "Epoch 90/100\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 6.1604\n",
      "Epoch 91/100\n",
      "2500/2500 [==============================] - 0s 27us/step - loss: 6.0927\n",
      "Epoch 92/100\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 6.0346\n",
      "Epoch 93/100\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 6.0378\n",
      "Epoch 94/100\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 6.0332\n",
      "Epoch 95/100\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 5.9936\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 0s 26us/step - loss: 5.9552\n",
      "Epoch 97/100\n",
      "2500/2500 [==============================] - 0s 27us/step - loss: 5.9693\n",
      "Epoch 98/100\n",
      "2500/2500 [==============================] - 0s 27us/step - loss: 5.8995\n",
      "Epoch 99/100\n",
      "2500/2500 [==============================] - 0s 28us/step - loss: 5.9071\n",
      "Epoch 100/100\n",
      "2500/2500 [==============================] - 0s 28us/step - loss: 5.8209\n"
     ]
    }
   ],
   "source": [
    "# Softmax layer training\n",
    "history_model = model.fit(x_train, y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.6026\n",
      "Test accuracy:  0.4564\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(model.predict(x_test), 1)\n",
    "print(\"Train accuracy: \", len(np.where(y_pred - y_test == 0)[0])/len(y_test))\n",
    "\n",
    "y_pred = np.argmax(model.predict(x_train), 1)\n",
    "print(\"Test accuracy: \", len(np.where(y_pred - y_train == 0)[0])/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output \"ove_layer_2\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"ove_layer_2\".\n"
     ]
    }
   ],
   "source": [
    "# For reproduceable results\n",
    "importlib.reload(custom_layers)\n",
    "importlib.reload(utilities)\n",
    "np.random.seed(1337)\n",
    "\n",
    "# Scale the KL term from the probabilistic layers for per-datapoint ELBO. \n",
    "# KL term added internally as a regularization term.\n",
    "scaled_kl = lambda q, p, _: tfp.distributions.kl_divergence(q, p) / tf.cast(train_samples, tf.float32)\n",
    "\n",
    "# OVE layer\n",
    "x_input =  tf.keras.Input((x_train.shape[1:]), name = 'x_input')\n",
    "y_input =  tf.keras.Input((1,), name = 'y_input')\n",
    "logits = custom_layers.OVELayer(num_samples=1, units=10, kernel_divergence_fn=scaled_kl)([x_input, y_input])\n",
    "model_ove = tf.keras.Model([x_input, y_input], logits)\n",
    "model_ove.compile(loss = None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 1s 272us/step - loss: 8.6083\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 0s 31us/step - loss: 8.5562\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 8.4496\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 0s 51us/step - loss: 8.3832\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 0s 29us/step - loss: 8.1992\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 0s 33us/step - loss: 8.2529\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 0s 30us/step - loss: 8.2164\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 0s 57us/step - loss: 8.0832\n",
      "Epoch 9/100\n",
      "2500/2500 [==============================] - 0s 32us/step - loss: 7.9489\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 7.9180\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 0s 55us/step - loss: 8.0651\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 7.9394\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 8.0572\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 0s 66us/step - loss: 7.8550\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 0s 69us/step - loss: 7.9818\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 7.6353\n",
      "Epoch 17/100\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 7.6570\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 7.6634\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 7.5916\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 7.5593\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 0s 61us/step - loss: 7.6006\n",
      "Epoch 22/100\n",
      "2500/2500 [==============================] - 0s 67us/step - loss: 7.3652\n",
      "Epoch 23/100\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 7.3966\n",
      "Epoch 24/100\n",
      "2500/2500 [==============================] - 0s 38us/step - loss: 7.5693\n",
      "Epoch 25/100\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 7.4116\n",
      "Epoch 26/100\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 7.3592\n",
      "Epoch 27/100\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 7.3706\n",
      "Epoch 28/100\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 7.2230\n",
      "Epoch 29/100\n",
      "2500/2500 [==============================] - 0s 100us/step - loss: 7.1865\n",
      "Epoch 30/100\n",
      "2500/2500 [==============================] - 0s 58us/step - loss: 7.1944\n",
      "Epoch 31/100\n",
      "2500/2500 [==============================] - 0s 81us/step - loss: 7.0448\n",
      "Epoch 32/100\n",
      "2500/2500 [==============================] - 0s 78us/step - loss: 7.0119\n",
      "Epoch 33/100\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 7.1319\n",
      "Epoch 34/100\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 6.9329\n",
      "Epoch 35/100\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 7.0319\n",
      "Epoch 36/100\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 7.0055\n",
      "Epoch 37/100\n",
      "2500/2500 [==============================] - 0s 56us/step - loss: 6.8805\n",
      "Epoch 38/100\n",
      "2500/2500 [==============================] - 0s 89us/step - loss: 6.9140\n",
      "Epoch 39/100\n",
      "2500/2500 [==============================] - 0s 70us/step - loss: 6.8448\n",
      "Epoch 40/100\n",
      "2500/2500 [==============================] - 0s 67us/step - loss: 6.7836\n",
      "Epoch 41/100\n",
      "2500/2500 [==============================] - 0s 49us/step - loss: 6.7438\n",
      "Epoch 42/100\n",
      "2500/2500 [==============================] - 0s 74us/step - loss: 6.6545\n",
      "Epoch 43/100\n",
      "2500/2500 [==============================] - 0s 57us/step - loss: 6.6981\n",
      "Epoch 44/100\n",
      "2500/2500 [==============================] - 0s 61us/step - loss: 6.6585\n",
      "Epoch 45/100\n",
      "2500/2500 [==============================] - 0s 38us/step - loss: 6.5336\n",
      "Epoch 46/100\n",
      "2500/2500 [==============================] - 0s 73us/step - loss: 6.6143\n",
      "Epoch 47/100\n",
      "2500/2500 [==============================] - 0s 77us/step - loss: 6.5630\n",
      "Epoch 48/100\n",
      "2500/2500 [==============================] - 0s 64us/step - loss: 6.4451\n",
      "Epoch 49/100\n",
      "2500/2500 [==============================] - 0s 64us/step - loss: 6.4693\n",
      "Epoch 50/100\n",
      "2500/2500 [==============================] - 0s 72us/step - loss: 6.4107\n",
      "Epoch 51/100\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 6.3287\n",
      "Epoch 52/100\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 6.2354\n",
      "Epoch 53/100\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 6.2471\n",
      "Epoch 54/100\n",
      "2500/2500 [==============================] - 0s 33us/step - loss: 6.1812\n",
      "Epoch 55/100\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 6.2544\n",
      "Epoch 56/100\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 6.0867\n",
      "Epoch 57/100\n",
      "2500/2500 [==============================] - 0s 84us/step - loss: 5.9893\n",
      "Epoch 58/100\n",
      "2500/2500 [==============================] - 0s 60us/step - loss: 6.0041\n",
      "Epoch 59/100\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 5.9747\n",
      "Epoch 60/100\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 6.1537\n",
      "Epoch 61/100\n",
      "2500/2500 [==============================] - 0s 26us/step - loss: 6.0724\n",
      "Epoch 62/100\n",
      "2500/2500 [==============================] - 0s 30us/step - loss: 6.0751\n",
      "Epoch 63/100\n",
      "2500/2500 [==============================] - 0s 25us/step - loss: 5.8681\n",
      "Epoch 64/100\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 5.8976\n",
      "Epoch 65/100\n",
      "2500/2500 [==============================] - 0s 51us/step - loss: 5.8736\n",
      "Epoch 66/100\n",
      "2500/2500 [==============================] - 0s 107us/step - loss: 5.6630\n",
      "Epoch 67/100\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 5.8330\n",
      "Epoch 68/100\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 5.6330\n",
      "Epoch 69/100\n",
      "2500/2500 [==============================] - 0s 71us/step - loss: 5.6137\n",
      "Epoch 70/100\n",
      "2500/2500 [==============================] - 0s 54us/step - loss: 5.6399\n",
      "Epoch 71/100\n",
      "2500/2500 [==============================] - 0s 52us/step - loss: 5.6302\n",
      "Epoch 72/100\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 5.5064\n",
      "Epoch 73/100\n",
      "2500/2500 [==============================] - 0s 49us/step - loss: 5.5198\n",
      "Epoch 74/100\n",
      "2500/2500 [==============================] - 0s 52us/step - loss: 5.6109\n",
      "Epoch 75/100\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 5.4895\n",
      "Epoch 76/100\n",
      "2500/2500 [==============================] - 0s 57us/step - loss: 5.4598\n",
      "Epoch 77/100\n",
      "2500/2500 [==============================] - 0s 57us/step - loss: 5.4468\n",
      "Epoch 78/100\n",
      "2500/2500 [==============================] - 0s 54us/step - loss: 5.4550\n",
      "Epoch 79/100\n",
      "2500/2500 [==============================] - 0s 53us/step - loss: 5.4679\n",
      "Epoch 80/100\n",
      "2500/2500 [==============================] - 0s 67us/step - loss: 5.1857\n",
      "Epoch 81/100\n",
      "2500/2500 [==============================] - 0s 65us/step - loss: 5.2569\n",
      "Epoch 82/100\n",
      "2500/2500 [==============================] - 0s 55us/step - loss: 5.1902\n",
      "Epoch 83/100\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 5.2474\n",
      "Epoch 84/100\n",
      "2500/2500 [==============================] - 0s 61us/step - loss: 5.1771\n",
      "Epoch 85/100\n",
      "2500/2500 [==============================] - 0s 52us/step - loss: 5.1377\n",
      "Epoch 86/100\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 5.1119\n",
      "Epoch 87/100\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 5.0392\n",
      "Epoch 88/100\n",
      "2500/2500 [==============================] - 0s 53us/step - loss: 5.0935\n",
      "Epoch 89/100\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 4.9260\n",
      "Epoch 90/100\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 4.9371\n",
      "Epoch 91/100\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 5.0860\n",
      "Epoch 92/100\n",
      "2500/2500 [==============================] - 0s 65us/step - loss: 4.8448\n",
      "Epoch 93/100\n",
      "2500/2500 [==============================] - 0s 59us/step - loss: 4.8864\n",
      "Epoch 94/100\n",
      "2500/2500 [==============================] - 0s 53us/step - loss: 4.8227\n",
      "Epoch 95/100\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 4.8711\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 0s 47us/step - loss: 4.8244\n",
      "Epoch 97/100\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 4.8819\n",
      "Epoch 98/100\n",
      "2500/2500 [==============================] - 0s 66us/step - loss: 4.8410: 0s - loss: 4.81\n",
      "Epoch 99/100\n",
      "2500/2500 [==============================] - 0s 72us/step - loss: 4.7052\n",
      "Epoch 100/100\n",
      "2500/2500 [==============================] - 0s 61us/step - loss: 4.7328\n"
     ]
    }
   ],
   "source": [
    "history_model = model_ove.fit(([x_train, y_train]),\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.5559\n",
      "Test accuracy:  0.3696\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(model_ove.predict([x_test, y_test]), 1)\n",
    "print(\"Train accuracy: \", len(np.where(y_pred - y_test == 0)[0])/len(y_test))\n",
    "\n",
    "y_pred = np.argmax(model_ove.predict([x_train, y_test]), 1)\n",
    "print(\"Test accuracy: \", len(np.where(y_pred - y_train == 0)[0])/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard:\n",
      "\n",
      "Accuracy on optimal decision:  0.6425\n",
      "Expected loss:  0.29891\n",
      "Standard:\n",
      "\n",
      "Accuracy on optimal decision:  0.5491\n",
      "Expected loss:  0.43961999999999996\n"
     ]
    }
   ],
   "source": [
    "#  Softmax model\n",
    "\n",
    "T = 10 \n",
    "yt_hat_std = np.array([model.predict([x_test]) for _ in range(T)])\n",
    "H_x_test_std = utilities.optimal_h(yt_hat_std,loss_mat) \n",
    "acc_std = accuracy_score(y_test, H_x_test_std)\n",
    "loss_std = np.mean(loss_mat[y_test, H_x_test_std])\n",
    "print('Standard:\\n')\n",
    "print('Accuracy on optimal decision: ', acc_std)\n",
    "print('Expected loss: ', loss_std)\n",
    "\n",
    "\n",
    "# OVE lower bound to softmax model\n",
    "\n",
    "T = 10 \n",
    "yt_hat_std = np.array([model_ove.predict([x_test, y_test]) for _ in range(T)])\n",
    "H_x_test_ove = utilities.optimal_h(yt_hat_std,loss_mat) \n",
    "acc_std = accuracy_score(y_test, H_x_test_ove)\n",
    "loss_std = np.mean(loss_mat[y_test, H_x_test_ove])\n",
    "print('Standard:\\n')\n",
    "print('Accuracy on optimal decision: ', acc_std)\n",
    "print('Expected loss: ', loss_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
